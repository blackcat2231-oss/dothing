import streamlit as st
import google.generativeai as genai
from PIL import Image
import pandas as pd
import json
from docx import Document
from docx.shared import Pt, RGBColor, Cm
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
import io
import time
import concurrent.futures

# --- 1. ç¶²é åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="ç¯¤è¡Œå¹¼å…’åœ’è©•é‡ç³»çµ±", layout="wide", page_icon="ğŸŒ±")

st.markdown("""
    <style>
    .main {background-color: #f9f9f9;}
    .stHeader {color: #2c3e50;}
    th {
        white-space: normal !important;
        background-color: #f0f2f6 !important;
    }
    td {text-align: center !important; vertical-align: middle !important;}
    td:last-child {text-align: left !important;}
    .stButton button { width: 100%; font-weight: bold; }
    </style>
    """, unsafe_allow_html=True)

# --- åˆå§‹åŒ– Session ---
if 'raw_records' not in st.session_state: st.session_state['raw_records'] = []
if 'class_df' not in st.session_state: st.session_state['class_df'] = pd.DataFrame()

# --- 2. å´é‚Šæ¬„ (å®‰å…¨ç‰ˆï¼šç„¡å¤–éƒ¨åœ–ç‰‡) ---
with st.sidebar:
    st.header("ğŸŒ± ç¯¤è¡Œå¹¼å…’åœ’") # æ”¹ç”¨æ–‡å­—æ¨™é¡Œ
    st.subheader("è©•é‡ç³»çµ± v2.6 (è¨ºæ–·ç‰ˆ)")
    
    # ç‹€æ…‹å„€è¡¨æ¿
    st.markdown("---")
    count = len(st.session_state['raw_records'])
    st.metric("ğŸ“Š æš«å­˜è³‡æ–™æ•¸", f"{count} ç­†")
    if count > 0:
        st.caption("âœ… è³‡æ–™å·²ä¿å­˜ï¼Œå¯ç”¢ç”Ÿå ±å‘Š")
    st.markdown("---")

    if "GEMINI_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        st.success("API é€£ç·šç‹€æ…‹ï¼šğŸŸ¢ ç·šä¸Š")
    else:
        st.error("âŒ API Key æœªè¨­å®š")
        st.stop()
        
    menu = st.radio("åŠŸèƒ½é¸å–®", ["ğŸ“ æ‰¹æ¬¡ä¸Šå‚³èˆ‡è¾¨è­˜", "ğŸ“„ ç”¢ç”Ÿæ•´åˆè©•é‡å ±å‘Š"])

# --- 3. æ ¸å¿ƒåŠŸèƒ½ ---

def get_fast_model():
    return genai.GenerativeModel('gemini-1.5-flash')

def analyze_single_image(image_file):
    model = get_fast_model()
    image = Image.open(image_file)
    
    prompt = """
    ä½ æ˜¯ä¸€ä½ç²¾æº–çš„è³‡æ–™è¼¸å…¥å“¡ã€‚é€™æ˜¯ä¸€å¼µå¹¼å…’åœ’è©•é‡è¡¨ã€‚
    
    ã€ä»»å‹™ä¸€ï¼šåˆ¤æ–·å­¸ç¿’å€ã€‘
    çœ‹è¡¨é ­æ–‡å­—ï¼Œåˆ¤æ–·æ˜¯å“ªå€‹å­¸ç¿’å€ (å¦‚:èªæ–‡å€,æ•¸å­¸å€...)ã€‚å­˜å…¥ "area"ã€‚

    ã€ä»»å‹™äºŒï¼šè®€å–æŒ‡æ¨™ã€‘
    è®€å–è¡¨æ ¼ä¸Šæ–¹é‚£ 4 å€‹æ¬„ä½æ¨™é¡Œã€‚

    ã€ä»»å‹™ä¸‰ï¼šåˆ¤æ–·åˆ†æ•¸ (åº§æ¨™å®šä½)ã€‘
    æ¯å€‹æ ¼å­å°æœ‰ "1 2 3 4"ã€‚è€å¸«åœˆé¸äº†ä¸€å€‹ã€‚
    è«‹çœ‹åœ“åœˆåœˆåœ¨å“ªè£¡ï¼š
    - åœˆåœ¨ 1 -> "A"
    - åœˆåœ¨ 2 -> "R"
    - åœˆåœ¨ 3 -> "D"
    - åœˆåœ¨ 4 -> "N"
    
    ã€å‚™è¨»ã€‘
    åˆä½µæ ¼å…§æ‰€æœ‰æ–‡å­—ï¼Œä¿ç•™ç·¨è™Ÿã€‚

    ã€è¼¸å‡º JSONã€‘
    {
      "area": "èªæ–‡å€",
      "headers": ["æŒ‡æ¨™1", "æŒ‡æ¨™2", "æŒ‡æ¨™3", "æŒ‡æ¨™4"],
      "students": [
        {"name": "å¹¼å…’ä¸€", "scores": ["A", "R", "A", "R"], "note": "å‚™è¨»..."},
        ...
      ]
    }
    """
    
    config = genai.types.GenerationConfig(temperature=0.0)
    
    # é‡è©¦æ©Ÿåˆ¶ (æ‡‰å° 429 éŒ¯èª¤)
    max_retries = 3
    last_error = ""
    
    for attempt in range(max_retries):
        try:
            response = model.generate_content([prompt, image], generation_config=config)
            
            # æ¸…æ½” JSON
            text = response.text
            if "```json" in text:
                text = text.replace("```json", "").replace("```", "")
            elif "```" in text:
                text = text.replace("```", "")
            
            return {"success": True, "data": json.loads(text.strip())}
            
        except Exception as e:
            last_error = str(e)
            if "429" in last_error: # å¡è»Šäº†
                time.sleep(3 * (attempt + 1)) # ä¼‘æ¯ä¹…ä¸€é»: 3ç§’, 6ç§’...
                continue
            else:
                return {"success": False, "error": last_error}
    
    return {"success": False, "error": f"é‡è©¦ {max_retries} æ¬¡å¾Œå¤±æ•—ã€‚åŸå› : {last_error}"}

def process_images_parallel(files):
    results = []
    errors = []
    
    # é™ä½ä½µç™¼æ•¸åˆ° 2ï¼Œé›–ç„¶æ…¢ä¸€é»é»ï¼Œä½†ä¿è­‰ä¸å¡è»Š
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_file = {executor.submit(analyze_single_image, f): f for f in files}
        
        bar = st.progress(0)
        info = st.empty()
        total = len(files)
        done = 0
        
        for future in concurrent.futures.as_completed(future_to_file):
            f = future_to_file[future]
            done += 1
            info.text(f"æ­£åœ¨åˆ†æ ({done}/{total}): {f.name}")
            bar.progress(done / total)
            
            outcome = future.result()
            if outcome["success"]:
                results.append(outcome["data"])
            else:
                errors.append(f"{f.name}: {outcome['error']}")
            
            # æ¯å¼µç…§ç‰‡è™•ç†å®Œç¨å¾®ä¼‘æ¯ä¸€ä¸‹
            time.sleep(1)
            
    info.empty()
    bar.empty()
    return results, errors

def generate_teacher_comments_fast(student_name, records):
    model = get_fast_model()
    data_text = f"å¹¼å…’ï¼š{student_name}\n"
    for r in records:
        data_text += f"[{r['area']}] å‚™è¨»:{r['note']}\n"
        data_text += f"æˆç¸¾:{[d['score'] for d in r['details']]}\n"

    prompt = f"""
    ä½ æ˜¯å¹¼å…’åœ’åœ’é•·ã€‚è«‹ç‚º {student_name} å¯«ä¸€ä»½ã€A4ç²¾ç°¡ç‰ˆã€‘è©•èªã€‚
    é™åˆ¶ï¼šç¸½å­—æ•¸ 200 å­—å…§ã€‚èªæ°£æº«æš–ã€‚
    æ ¼å¼ JSONï¼š
    {{ "observation": "è§€å¯Ÿ...", "suggestion": "å»ºè­°..." }}
    """
    config = genai.types.GenerationConfig(temperature=0.7)
    try:
        response = model.generate_content(prompt, generation_config=config)
        text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(text)
    except:
        return {"observation": "è«‹è¦ªå¸«å¤šåŠ æºé€šã€‚", "suggestion": "é™ªä¼´æ˜¯æœ€å¥½çš„ç¦®ç‰©ã€‚"}

def create_word_report(grouped_data):
    doc = Document()
    # è¨­å®šé‚Šç•Œ
    for section in doc.sections:
        section.top_margin = Cm(1.27)
        section.bottom_margin = Cm(1.27)
        section.left_margin = Cm(1.27)
        section.right_margin = Cm(1.27)
    
    # è¨­å®šå­—å‹
    style = doc.styles['Normal']
    style.font.name = 'Microsoft JhengHei'
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft JhengHei')
    style.font.size = Pt(10)
    
    bar = st.progress(0)
    status = st.empty()
    total = len(grouped_data)
    
    for idx, (name, records) in enumerate(grouped_data.items()):
        status.text(f"æ’°å¯«å ±å‘Š ({idx+1}/{total}): {name} ...")
        bar.progress((idx+1)/total)
        
        if idx > 0: doc.add_page_break()
        
        # æ¨™é¡Œ
        head = doc.add_heading('ç¯¤è¡Œéç‡Ÿåˆ©å¹¼å…’åœ’  å¹¼å…’å­¸ç¿’å€å€‹åˆ¥è©•é‡å ±å‘Š', 0)
        head.alignment = WD_ALIGN_PARAGRAPH.CENTER
        head.style.font.size = Pt(16)
        
        p = doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        run = p.add_run(f"å¹¼å…’å§“åï¼š{name}     æ—¥æœŸï¼š2026å¹´___æœˆ___æ—¥")
        run.bold = True
        run.font.size = Pt(12)
        
        table = doc.add_table(rows=1, cols=2)
        table.style = 'Table Grid'
        
        hdr = table.rows[0].cells
        hdr[0].text = "å„å€å­¸ç¿’æŒ‡æ¨™å…§å®¹"
        hdr[1].text = "çµæœ"
        table.columns[0].width = Cm(14)
        table.columns[1].width = Cm(3)
        
        for r in records:
            row = table.add_row().cells
            row[0].merge(row[1])
            p_area = row[0].paragraphs[0]
            run_area = p_area.add_run(f"â–  {r['area']}")
            run_area.bold = True
            run_area.font.color.rgb = RGBColor(0, 51, 102)
            
            for item in r['details']:
                row_item = table.add_row().cells
                row_item[0].text = item['idx']
                row_item[0].paragraphs[0].paragraph_format.left_indent = Cm(0.5)
                row_item[1].text = item['score']
                row_item[1].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        doc.add_paragraph("")
        comments = generate_teacher_comments_fast(name, records)
        
        doc.add_paragraph("ã€è€å¸«çš„è§€å¯Ÿã€‘").runs[0].bold = True
        doc.add_paragraph(comments['observation'])
        doc.add_paragraph("ã€å±…å®¶äº’å‹•å°æ’‡æ­¥ã€‘").runs[0].bold = True
        doc.add_paragraph(comments['suggestion'])
        
        footer = doc.add_paragraph("è©•é‡ä»£è™Ÿï¼šA(ä¸»å‹•ç†Ÿç·´) R(è¡¨ç¾è‰¯å¥½) D(ç™¼å±•ä¸­) N(éœ€å”åŠ©)")
        footer.alignment = WD_ALIGN_PARAGRAPH.CENTER
        footer.runs[0].font.color.rgb = RGBColor(128,128,128)
        footer.runs[0].font.size = Pt(9)

    bar.empty()
    status.empty()
    
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- 4. ä¸»é é¢ ---

if menu == "ğŸ“ æ‰¹æ¬¡ä¸Šå‚³èˆ‡è¾¨è­˜":
    st.title("ğŸ“ æ‰¹æ¬¡è™•ç† (v2.6 è¨ºæ–·ç‰ˆ)")
    st.info("ğŸ’¡ ç¢ºä¿ç©©å®šï¼šå·²é™ä½å¹³è¡Œè™•ç†æ•¸é‡ï¼Œé¿å… API å¡è»Šã€‚")
    
    files = st.file_uploader("é¸æ“‡ç…§ç‰‡ (å…¨é¸)", type=['jpg','png','jpeg'], accept_multiple_files=True)
    
    if files and st.button("ğŸš€ é–‹å§‹åˆ†æ"):
        results, errors = process_images_parallel(files)
        
        # 1. è™•ç†æˆåŠŸçš„éƒ¨åˆ†
        if results:
            all_data = []
            raw_records = []
            for res in results:
                area = res.get("area","æœªçŸ¥")
                headers = res.get("headers", ["I1","I2","I3","I4"])
                for s in res.get("students", []):
                    row = {"å¹¼å…’å§“å":s.get("name"), "å­¸ç¿’å€":area}
                    scores = s.get("scores", [])
                    details = []
                    for i, sc in enumerate(scores):
                        if i < 4: 
                            h_name = headers[i] if i < len(headers) else f"æŒ‡æ¨™{i+1}"
                            row[h_name] = sc
                            details.append({"idx": h_name, "score": sc})
                    row["å‚™è¨»"] = s.get("note")
                    all_data.append(row)
                    raw_records.append({
                        "name": s.get("name"),
                        "area": area,
                        "details": details,
                        "note": s.get("note")
                    })
            
            # è¿½åŠ è³‡æ–™
            if 'raw_records' not in st.session_state: st.session_state['raw_records'] = []
            st.session_state['raw_records'].extend(raw_records)
            
            # æ›´æ–°é¡¯ç¤ºè¡¨æ ¼
            if 'class_df' not in st.session_state: st.session_state['class_df'] = pd.DataFrame()
            new_df = pd.DataFrame(all_data)
            st.session_state['class_df'] = pd.concat([st.session_state['class_df'], new_df], ignore_index=True)
            
            st.success(f"âœ… æˆåŠŸè™•ç† {len(results)} å¼µç…§ç‰‡ï¼")
        
        # 2. è™•ç†å¤±æ•—çš„éƒ¨åˆ† (è¨ºæ–·å ±å‘Š)
        if errors:
            st.error(f"âš ï¸ æœ‰ {len(errors)} å¼µç…§ç‰‡è™•ç†å¤±æ•—ï¼ŒåŸå› å¦‚ä¸‹ï¼š")
            for err in errors:
                st.code(err) # é¡¯ç¤ºçœŸå¯¦éŒ¯èª¤è¨Šæ¯
                if "429" in err:
                    st.warning("ğŸ‘‰ æç¤ºï¼š429 ä»£è¡¨ Google API å¿™ç¢Œä¸­ï¼Œè«‹ç­‰å¾… 1 åˆ†é˜å¾Œå†è©¦ã€‚")

    if not st.session_state['class_df'].empty:
        st.dataframe(st.session_state['class_df'])

elif menu == "ğŸ“„ ç”¢ç”Ÿæ•´åˆè©•é‡å ±å‘Š":
    st.title("ğŸ“„ å ±å‘Šç”Ÿæˆ")
    
    if st.session_state['raw_records']:
        grouped = {}
        for r in st.session_state['raw_records']:
            name = r['name']
            if name not in grouped: grouped[name] = []
            grouped[name].append(r)
        
        st.write(f"ğŸ“š è³‡æ–™åº«å°±ç·’ï¼šå…± {len(grouped)} ä½å¹¼å…’è³‡æ–™ã€‚")

        if st.button("âœ¨ é»æ“Šé€™è£¡ç”¢ç”Ÿ Word æª”"):
            with st.spinner("AI åœ’é•·æ­£åœ¨å‹•ç­†å¯«è©•èª..."):
                doc_file = create_word_report(grouped)
                st.session_state['generated_doc'] = doc_file.getvalue()
                st.success("å ±å‘Šç”¢ç”Ÿå®Œç•¢ï¼")
        
        if 'generated_doc' in st.session_state:
            st.download_button(
                label="ğŸ“¥ é»æˆ‘ä¸‹è¼‰ Word è©•é‡å ±å‘Š",
                data=st.session_state['generated_doc'],
                file_name="ç¯¤è¡Œå¹¼å…’åœ’_è©•é‡å ±å‘Š_v2.6.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
            
    else:
        st.warning("âš ï¸ æš«å­˜å€æ˜¯ç©ºçš„ï¼è«‹å…ˆå›ä¸Šä¸€é ä¸Šå‚³ä¸¦åˆ†æç…§ç‰‡ã€‚")
